{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Candidates and Queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidates and Queries are:\n",
      "No of candidates: 218753\n",
      "No of queries: 3046\n",
      "Size of vocab: 220018\n",
      "No of unigrams: 161745\n",
      "No of bigrams: 51622\n",
      "No of trigrams: 6651\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "\n",
    "def normalize_term(term):\n",
    "    \"\"\" Normalize a query or candidate: lower-case and replace spaces with underscores.\"\"\"\n",
    "    \n",
    "    return term.lower().replace(\" \", \"_\")\n",
    "\n",
    "def load_queries(file, normalize=True):\n",
    "    \"\"\"Given query file, return list of queries and list of query types.\n",
    "    \n",
    "    \"\"\"\n",
    "    f=open(file)                                  #Opening given file\n",
    "    queries = []   \n",
    "    query_types = []\n",
    "    for line in f:                                    \n",
    "        q, qtype = line.strip().split(\"\\t\")      #Seperating the given query word and its type from file\n",
    "        if normalize:                            #Checking if query word is normalized\n",
    "            q = normalize_term(q)                #Normalize the query word \n",
    "        queries.append(q)                        #Appending query word in a list \n",
    "        query_types.append(qtype)                #Appending query type in another list\n",
    "    return queries, query_types\n",
    "\n",
    "\n",
    "\n",
    "def load_candidates(file, normalize=True):\n",
    "    \"\"\"Given a file, return list of candidates.\n",
    "    \n",
    "    \"\"\"\n",
    "    with codecs.open(file, \"r\", encoding=\"utf-8\") as f:      #Opening file with unicode strings with codecs\n",
    "        candidates = []\n",
    "        for line in f:                                    \n",
    "            a = line.strip()                                \n",
    "            if len(a):\n",
    "                if normalize:\n",
    "                    a = normalize_term(a)\n",
    "                candidates.append(a)                        #Appending candidate hypernym to list\n",
    "        return candidates\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "def load_vocab(file, lower_queries=False):\n",
    "    \"\"\" Given a data file load candidates and queries (training, trial, and test). \n",
    "    Return set of candidates and set of queries.\n",
    "    \"\"\"\n",
    "    candidates = set(load_candidates(file, normalize=False))\n",
    "    \n",
    "    list1=[]                                                   #Collecting all query words in single list\n",
    "    f1=load_queries(\"1A.english.test.data.txt\")  \n",
    "    f2=load_queries(\"1A.english.training.data.txt\")\n",
    "    f3=load_queries(\"1A.english.trial.data.txt\")\n",
    "\n",
    "    for i in [f1,f2,f3]:\n",
    "        list1.extend(i[0])\n",
    "    \n",
    "    queries=set(list1)\n",
    "    return queries, candidates\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"Candidates and Queries are:\")\n",
    "queries, candidates = load_vocab('1A.english.vocabulary.txt') \n",
    "print(\"No of candidates: {}\".format(len(candidates)))\n",
    "print(\"No of queries: {}\".format(len(queries)))\n",
    "\n",
    "vocab = candidates.union(queries)                                #Collecting candidates and queries in single list\n",
    "print(\"Size of vocab: {}\".format(len(vocab)))\n",
    "trigrams = set()\n",
    "bigrams = set()\n",
    "unigrams = set()\n",
    "for i in vocab:\n",
    "    Num_words = len(i.split())\n",
    "    if Num_words == 3:\n",
    "        trigrams.add(i)                                        #Creating trigram list from vocab\n",
    "    elif Num_words == 2:\n",
    "        bigrams.add(i)                                         #Creating bigram list from vocab\n",
    "    elif Num_words == 1:\n",
    "        unigrams.add(i)\n",
    "    else:\n",
    "        msg = \"Error: '{}' is not unigram, bigram or trigram\".format(term)\n",
    "        raise ValueError(msg)                                  #Raising error if no n-grams found\n",
    "print(\"No of unigrams: {}\".format(len(unigrams)))\n",
    "print(\"No of bigrams: {}\".format(len(bigrams)))\n",
    "print(\"No of trigrams: {}\".format(len(trigrams)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting lines in corpus...\n",
      "209\n",
      "Counting n-gram frequencies in corpus...\n",
      "209/209 lines processed. Vocab coverage: 792/220018.\n",
      "Nb zero-frequency queries: 2987\n",
      "Nb zero-frequency candidates: 217974\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "from collections import defaultdict\n",
    "\n",
    "print(\"Counting lines in corpus...\")\n",
    "f10=codecs.open('Short_corpus.txt',\"r\", encoding=\"utf-8\")      #Reading file using codecs\n",
    "Num_lines = sum(1 for line in f10)                             #Counting number of lines in file\n",
    "\n",
    "print(\"Counting n-gram frequencies in corpus...\")\n",
    "term_to_freq_in = defaultdict(int)                             \n",
    "line_count = 0\n",
    "with codecs.open('Short_corpus.txt', \"r\", encoding=\"utf-8\") as f_1:\n",
    "    for line in f_1:\n",
    "        line_count += 1                                       #Counting lines in file\n",
    "        if line_count % 100000 == 0:                          #Checking for every million lines\n",
    "            msg = \"{}/{} lines processed.\".format(line_count, Num_lines)          \n",
    "            msg += \" Vocab coverage: {}/{}.\".format(len(term_to_freq_in), len(vocab))\n",
    "            print(msg)                                         #Printing processed lines\n",
    "        line = line.strip().replace(\"_\", \"\")         \n",
    "        words = [w.lower() for w in line.split()]               #Adding lines into list\n",
    "        for n in [1,2,3]:\n",
    "            for i in range(len(words)+n-1):\n",
    "                term = \" \".join(words[i:i+n])\n",
    "                if term in vocab:\n",
    "                    term_to_freq_in[term] += 1                 #Calculating frequency of each word\n",
    "msg = \"{}/{} lines processed.\".format(line_count, Num_lines)\n",
    "msg += \" Vocab coverage: {}/{}.\".format(len(term_to_freq_in), len(vocab))\n",
    "print(msg)\n",
    "No_missing_q = sum(1 for w in queries if term_to_freq_in[w] == 0)       #Print if querries are not found in corpus\n",
    "No_missing_c = sum(1 for w in candidates if term_to_freq_in[w] == 0)    #Print if candidates are not found in corpus\n",
    "print(\"Nb zero-frequency queries: {}\".format(No_missing_q))\n",
    "print(\"Nb zero-frequency candidates: {}\".format(No_missing_c))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_ngrams(tokens, n, ngram_vocab, term_to_freq):\n",
    "    \"\"\" Given a list of tokens and a vocab of n-grams, extract list of\n",
    "    non-overlapping n-grams found in tokens.\n",
    "    \"\"\"\n",
    "    ngrams_found = []                                  \n",
    "    for i in range(len(tokens)-n+1):                 #Looking in range of tokens\n",
    "        term = \" \".join(tokens[i:i+n])               \n",
    "        if term in ngram_vocab:                     \n",
    "            ngrams_found.append((i,term))\n",
    "    if len(ngrams_found) < 2:\n",
    "        return ngrams_found\n",
    "    \n",
    "    ngrams_filtered = ngrams_found[:1]               # Eliminating overlap\n",
    "    for (start, term) in ngrams_found[1:]:           #Looking for n-grams in start and end term list\n",
    "        prev_start, prev_term = ngrams_filtered[-1]\n",
    "        if start - prev_start < n:\n",
    "            if term not in term_to_freq or term_to_freq[term] < term_to_freq[prev_term]:\n",
    "                ngrams_filtered[-1] = (start, term)\n",
    "        else:\n",
    "            ngrams_filtered.append((start, term))\n",
    "    return ngrams_filtered\n",
    "\n",
    "def get_formatted_sample(strings, max_sampled):\n",
    "    sub = strings[:max_sampled]\n",
    "    if len(strings) > max_sampled:\n",
    "        sub.append(\"... ({}) more\".format(len(strings)-max_sampled))\n",
    "    return \", \".join(sub)\n",
    "\n",
    "def get_indices_unmasked_spans(mask):\n",
    "    \"\"\" Given a mask array, return spans of unmasked list items.\"\"\"\n",
    "    spans = []\n",
    "    start = 0\n",
    "    while start < len(mask):\n",
    "        if mask[start]:\n",
    "            start += 1\n",
    "            continue\n",
    "        end = start\n",
    "        for i in range(start+1, len(mask)):\n",
    "            if mask[i]:\n",
    "                break\n",
    "            else:\n",
    "                end = i\n",
    "        spans.append((start, end))\n",
    "        start = end + 1\n",
    "    return spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing corpus...\n",
      "209/209 lines processed. Vocab coverage: 763/220018.\n",
      "Nb missing queries in output: 2994\n",
      "Examples: 20th_century, 25th_hour, 9mm, a1_motorway, aac, aare, aaron_copland, abdication, abhorrence, abnegation, about.com, abutilon_theophrasti, academic_year, academician, acanthocephalan, accelerator, acceptance, accessibility, accolade, accounting, accreditation, achille, acquiescence, acquittance, acre, act_of_congress, activator, actor, actuator, ada_comstock, adage, adam_crosthwaite, adams_express_company, administrator, admittance, adobe, adobe_flash_lite, adult, advancement, adventure, advocacy, aerodrome, aerofoil, aesculapius, affection, affectionateness, aflatoxin, africa, afterlife, aftermath, agar, agitation, agogo, agriculture, agrippina, ai, aileron, air_mass, aircraft, airlock, airport, aksu, alb, albuquerque, aldicarb, alec_stewart, alex_sink, alexandre_gustave_eiffel, algebra, allakhazam, allies, alluvium, alpha_omega_epsilon, altai_mountains, alternative_tentacles, altivec, alumnus, amaretto, amazon, amelia_earhart, american_stock_exchange, amis, ammonium, amoy, amsterdam_admirals, amsterdam_internet_exchange, anal_sex, analytical_engine, anapsid, ancient_philosophy, andrew_anderson, andrew_mccormick, andronicus_of_rhodes, anglophile, angostura_reservoir, angular_momentum, animation, animorphs, animosity, anna_lindh, anna_magnani, annals, annual_general_meeting, annual_report, ansel_adams, antagonism, antenna, anthony_caro, anthony_hopkins, antibody, antichrist, anticlimax, anticyclone, antioxidant, antony_leung, anycast, apartment, ape_escape, apiaceae, apparition, appearing, apple_pie, aquamarine, aqueduct, aram_i, aramaic, arapaho, arbitrage, arboriculture, archaean, archduke, archeologist, architecture, argument, ari_up, aridity, arm, arms, army, artemisia, arthur_ashe, article, aryabhata, asia, assertion, assignment, assist, assistance, asterism, at&t, atomizer, attempt, attenuator, attic, attorney, audrey_hepburn, augmentation, august, auguste_rodin, augustine_volcano, austin, austral, authentication, authoress, authoritarian_regime, autocracy, autograph, automatix, autonomous_region, autoresponder, availability, avant-garde, avenue, avulsion, award, axile_placentation, azathioprine, azua, baba, baby_tears, babysitting, bachelor, back_porch, backstay, backwash, baguette, baker, balance_of_power, balk, ball, balsa_raft, balsam, bank, banyan, baptismal_font, bar, barley, barney_bigard, baronet, baronetcy, ... (2794) more\n",
      "Nb missing candidates in output: 218003\n",
      "Examples: '39, '77, '78, 'd, 're, 've, ( ), +1, +15, +7, -40, .02, .22, .23, .303, .32, .357, .38, .460, 0, 0-0, 0-0-0, 0-3, 0.0.0.0, 0/3, 0/6, 00, 001, 002, 003, 004, 005, 008, 009, 00s, 01.10, 010, 0103, 0110, 0114, 013, 0191, 02, 020, 03, 030, 0304, 033, 04, 05/03, 06, 06/05, 060, 07, 08, 0800 number, 09, 09l, 0a, 0b, 0c, 0d, 0e, 0f, 0h, 0k, 0l, 0n, 0r, 0s, 0th, 0v, 0x, 0x80, 0z, 1, 1 august, 1 cent, 1 gauge, 1 july, 1 kilometre, 1 kings, 1 november, 1 vs. 100, 1,000,000,000, 1,3-butadiene, 1,3-dipolar cycloaddition, 1,5-cyclooctadiene, 1-1, 1-2-3, 1-3, 1-800, 1-800-flowers, 1-9, 1-900, 1-bromopropane, 1-click, 1-connected, 1-hexanol, 1-hexene, 1-methylcyclopropene, 1-methylnaphthalene, 1-naphthol, 1-propanol, 1-up, 1.0, 1.26, 1.96, 1/2, 1/3, 1/6, 1/7, 1/8, 1/c, 10, 10 cents, 10 nanometer, 10,000 metres, 10,000 yen note, 10-20, 10-20-life, 10.5, 10.9, 10/18, 100, 100 kilometres, 100 metres, 1000, 1000 bc, 10000, 100000, 1000000, 10000000, 100000000, 1000s, 1000th, 1000x, 1001, 1002, 1003, 1004, 10048, 1005, 1006, 1007, 1008, 10080, 1009, 100f, 100s, 100th, 100th meridian west, 101, 101 series, 1010, 10101, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1018, 1019, 101st, 102, 1020, 1020s, 1021, 1022, 1023, 1024, 1025, 1026, 1027, 1028, 1029, 103, 1030, 1030s, 1031, 1032, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 104, 1040, 1040s, 1041, 1042, 1043, 1044, 1045, 1046, 1047, ... (217803) more\n",
      "Wrote vocab --> output_file.txt\n",
      "Wrote vocab --> output_file.txt\n"
     ]
    }
   ],
   "source": [
    "#print(\"Counting lines in corpus...\")\n",
    "f10=codecs.open('Short_corpus.txt',\"r\", encoding=\"utf-8\")       #Open corpus file   \n",
    "Num_lines = sum(1 for line in f10)                              #Counting number of lines\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "# Replace multi-word terms with single tokens\n",
    "print(\"\\nProcessing corpus...\")\n",
    "term_to_freq_out = defaultdict(int)\n",
    "line_count = 0\n",
    "with codecs.open('Short_corpus.txt', \"r\", encoding=\"utf-8\") as f_in, codecs.open('output_file.txt', \"w\", encoding=\"utf-8\") as f_out:\n",
    "    for line in f_in:                                         #For lines in corpus\n",
    "        line_count += 1                                       #Counting the line in corpus\n",
    "        if line_count % 100000 == 0:                          #Keeping track of line in corpus\n",
    "            msg = \"{}/{} lines processed.\".format(line_count, Num_lines)\n",
    "            msg += \" Vocab coverage: {}/{}.\".format(len(term_to_freq_out), len(vocab))\n",
    "            print(msg)\n",
    "        line = line.strip().replace(\"_\", \"\")                 #Separating lines with comma\n",
    "        words = [w.lower() for w in line.split()]            #Adding lines into list words    \n",
    "        \n",
    "        \n",
    "        term_lengths = [0 for _ in range(len(words))] #Making list indicating the length of the term found at each position    \n",
    "        masked_indices = [0 for _ in range(len(words))]\n",
    "        \n",
    "        # Check for trigrams\n",
    "        trigrams_found = extract_ngrams(words, 3, trigrams, term_to_freq_in) #Extracting trigrams from list words\n",
    "        for (i, term) in trigrams_found:                                     \n",
    "            term_lengths[i] = 3                             \n",
    "            term_to_freq_out[term] += 1\n",
    "            masked_indices[i] = 1\n",
    "            masked_indices[i+1] = 1\n",
    "            masked_indices[i+2] = 1\n",
    "        \n",
    "        # Check for bigrams\n",
    "        for (beg, end) in get_indices_unmasked_spans(masked_indices):    \n",
    "            bigrams_found = extract_ngrams(words[beg:end+1], 2, bigrams, term_to_freq_in)                \n",
    "            for (i, term) in bigrams_found:\n",
    "                term_lengths[beg+i] = 2\n",
    "                term_to_freq_out[term] += 1\n",
    "                masked_indices[beg+i] = 1\n",
    "                masked_indices[beg+i+1] = 1\n",
    "        \n",
    "        # Check for unigrams\n",
    "        for (beg, end) in get_indices_unmasked_spans(masked_indices):    \n",
    "            for i in range(beg,end+1):\n",
    "                term = words[i]\n",
    "                if term in unigrams:\n",
    "                    term_to_freq_out[term] += 1\n",
    "                    term_lengths[i] = 1\n",
    "        \n",
    "        \n",
    "        norm_terms = []                             # Writing sentence in output file\n",
    "        i = 0\n",
    "        while i < len(term_lengths):\n",
    "            n = term_lengths[i]                    #Checking the length of term in list term_length\n",
    "            if n > 1:\n",
    "                norm_terms.append(\"_\".join(words[i:i+n]))  \n",
    "                i += n\n",
    "            else:\n",
    "                if n == 0:                        #Checking if n is zero or not\n",
    "                    norm_term = \"<UNK>\"           #If zero add unknown to list\n",
    "                else:\n",
    "                    norm_term = words[i]\n",
    "                norm_terms.append(norm_term)\n",
    "                i += 1\n",
    "        sent = \" \".join(norm_terms)             #Joining norm terms\n",
    "        f_out.write(sent+\"\\n\")                  #Writing in output file\n",
    "        \n",
    "msg = \"{}/{} lines processed.\".format(line_count, Num_lines)             #Keeping track of processed lines\n",
    "msg += \" Vocab coverage: {}/{}.\".format(len(term_to_freq_out), len(vocab))\n",
    "print(msg)\n",
    "\n",
    "missing_q = [w for w in queries if term_to_freq_out[w] == 0]       #Checking for missed queries\n",
    "missing_c = [w for w in candidates if term_to_freq_out[w] == 0]    #Checking for missed candidates\n",
    "print(\"Nb missing queries in output: {}\".format(len(missing_q)))\n",
    "max_shown = 200\n",
    "if len(missing_q):\n",
    "    msg = \"Examples: {}\".format(get_formatted_sample(sorted(missing_q), max_shown))  #Print examples of missing queries\n",
    "    print(msg)\n",
    "print(\"Nb missing candidates in output: {}\".format(len(missing_c)))                #Print examples of missing candidate\n",
    "if len(missing_c):\n",
    "    msg = \"Examples: {}\".format(get_formatted_sample(sorted(missing_c), max_shown))\n",
    "    print(msg)\n",
    "\n",
    "\n",
    "# Write frequencies\n",
    "with open('output_file.txt', \"w\", encoding=\"utf-8\") as f:              #Writing in output processed file\n",
    "    for term, freq in sorted(term_to_freq_out.items(), key=lambda x:x[0]):\n",
    "        # Normalize term\n",
    "        term_norm = \"_\".join(term.split())\n",
    "        f.write(\"{}\\t{}\\n\".format(term_norm, freq))\n",
    "msg = \"Wrote vocab --> {}\".format('output_file.txt')\n",
    "print(msg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
